{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all functions necessary for the notebook\n",
    "import csv\n",
    "from math import sqrt\n",
    "from random import randrange\n",
    "from random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Loading, and conversion of CSV file #####\n",
    "\n",
    "## Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "## Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "## Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "\n",
    "##### Normalize Data ###########\n",
    "\n",
    "# Find the min and max values for each column\n",
    "\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        colvalues = [row[i] for row in dataset]\n",
    "        min_value = min(colvalues) \n",
    "        max_value = max(colvalues)\n",
    "        minmax.append([min_value, max_value])\n",
    "    return minmax\n",
    "\n",
    "# Normalize the dataset except last row for classification values\n",
    "def Normalize_Dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Split dataset into k folds and algorithm procedure\n",
    "\n",
    "# Split a dataset into $k$ folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Evaluate an algorithm using a cross-validation split\n",
    "def evaluate_algorithm_cv(dataset, algorithm, n_folds, performance_assessment, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        performance = performance_assessment(actual, predicted)\n",
    "        scores.append(performance)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Performance measure #####\n",
    "\n",
    "# Get accuracy of prediction #\n",
    "def getAccuracy(actual,predicted):\n",
    "    correct = 0\n",
    "    for x in range(len(actual)):\n",
    "        if actual[x] == predicted[x]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(dataset, ratio):\n",
    "    sample = list()\n",
    "    n_sample = round(len(dataset) * ratio)\n",
    "    while len(sample) < n_sample:\n",
    "        index = randrange(len(dataset))\n",
    "        sample.append(dataset[index])\n",
    "    return sample\n",
    "\n",
    "# GINI index as cost function to minimize\n",
    "def gini_index(groups,classes):\n",
    "    #count all samples at split point\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    #Gini indexes weighted sum\n",
    "    Gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        if size == 0.0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        #score group based on score of each class\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size\n",
    "            score += p*p\n",
    "        #weight group score b yrelative size\n",
    "        Gini += (1.0-score) * (size / n_instances)\n",
    "    return Gini\n",
    "\n",
    "#Split a dataset\n",
    "def test_split(index,value,dataset):\n",
    "    left, right = list(), list()\n",
    "    for row in dataset:\n",
    "        if row[index]< value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "#Best splitpoint for dataset with choice of attributes to avoid redundancy\n",
    "def get_split_forest(dataset, n_attributes):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    attributes = list()\n",
    "    while len(attributes) < n_attributes:\n",
    "        index = randrange(len(dataset[0])-1)\n",
    "        if index not in attributes:\n",
    "            attributes.append(index)\n",
    "    for index in attributes:\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            Gini = gini_index(groups, class_values)\n",
    "            #print('X%d < %.3f Gini=%.3f' % ((index+1), row[index], Gini))\n",
    "            if Gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], Gini, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "\n",
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "# Create a function to split a node in function of diffrent parameters\n",
    "def split_node_forest(node, max_depth, min_size, n_attributes, depth):\n",
    "    left, right = node['groups']\n",
    "    #Delete data from node as it is no longer needed\n",
    "    del(node['groups'])\n",
    "    #Check whether left node or right node is empty to create a terminal node\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    #Check if we have reached the maximum depth of the tree --> Create terminal node\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    #Process on the both children, by checking if min size is reached first or further split of tree is required\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split_forest(left, n_attributes)\n",
    "        split_node_forest(node['left'], max_depth, min_size, n_attributes,depth+1)\n",
    "    #Right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split_forest(right, n_attributes)\n",
    "        split_node_forest(node['right'], max_depth, min_size, n_attributes,  depth+1)\n",
    "\n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, n_attributes ,min_size):\n",
    "    root = get_split_forest(train, n_attributes)\n",
    "    split_node_forest(root, max_depth, min_size, n_attributes ,1)\n",
    "    return root\n",
    "\n",
    "# Print a decision tree\n",
    "def print_tree(node, depth=0):\n",
    "    if isinstance(node, dict):\n",
    "        print('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n",
    "        print_tree(node['left'], depth+1)\n",
    "        print_tree(node['right'], depth+1)\n",
    "    else:\n",
    "        print('%s[%s]' % ((depth*' ', node)))\n",
    "\n",
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "# Make a prediction with a list of bagged trees\n",
    "def bagging_predict(trees, row):\n",
    "    predictions = [predict(tree, row) for tree in trees]\n",
    "    return max(set(predictions), key=predictions.count)\n",
    "\n",
    "\n",
    "\n",
    "# Random Forest Algorithm\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "    trees = list()\n",
    "    for _ in range(n_trees):\n",
    "        sample = subsample(train, sample_size)\n",
    "        tree = build_tree(sample, max_depth, min_size, n_features)\n",
    "        trees.append(tree)\n",
    "    predictions = [bagging_predict(trees, row) for row in test]\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trees: 1\n",
      "Scores: [55.00000000000001, 70.0, 40.0, 60.0, 80.0, 70.0, 75.0, 50.0, 80.0, 80.0]\n",
      "Mean Accuracy: 66.000%\n",
      "Trees: 5\n",
      "Scores: [70.0, 65.0, 90.0, 80.0, 65.0, 65.0, 80.0, 80.0, 75.0, 55.00000000000001]\n",
      "Mean Accuracy: 72.500%\n",
      "Trees: 10\n",
      "Scores: [75.0, 50.0, 80.0, 65.0, 65.0, 80.0, 85.0, 75.0, 85.0, 65.0]\n",
      "Mean Accuracy: 72.500%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Test the random forest algorithm on sonar dataset\n",
    "    seed(2)\n",
    "    # load and prepare data\n",
    "    filename = 'sonar-all-data.csv'\n",
    "    dataset = load_csv(filename)\n",
    "    # convert string attributes to integers\n",
    "    for i in range(0, len(dataset[0])-1):\n",
    "        str_column_to_float(dataset, i)\n",
    "    # convert class column to integers\n",
    "    str_column_to_int(dataset, len(dataset[0])-1)\n",
    "    # evaluate algorithm\n",
    "    n_folds = 10\n",
    "    max_depth = 10\n",
    "    min_size = 1.0\n",
    "    sample_size = 3.0\n",
    "    n_features = int(sqrt(len(dataset[0])-1))\n",
    "    for n_trees in [1, 5, 10]:\n",
    "        scores = evaluate_algorithm_cv(dataset, random_forest, n_folds, getAccuracy,max_depth, min_size, sample_size, n_trees, n_features)\n",
    "        print('Trees: %d' % n_trees)\n",
    "        print('Scores: %s' % scores)\n",
    "        print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
